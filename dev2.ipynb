{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models import GPT\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "from utils import BatchLoader, estimate_loss, train_loop, load, save\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16  # number of independent sequences that'll be processed in parallel\n",
    "block_size = 128  # maximum context length for the preds\n",
    "max_iters = 1000\n",
    "eval_interval = 200\n",
    "learning_rate = 3e-4\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "eval_iters = 200\n",
    "n_embd = 256\n",
    "n_head = 4\n",
    "n_blocks = 4\n",
    "dropout = 0.2\n",
    "# --------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# data preparation\n",
    "text = open(\"dataset/tinyshakespeare.txt\", \"r\").read()\n",
    "# set up the vocabulary\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "tokenizer = Tokenizer(chars)\n",
    "\n",
    "data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "\n",
    "n = int(0.9 * len(data))  # first 90% will be the training set\n",
    "n1 = int(0.95 * len(data))  # 90-95% will be the validation set and the last 5% will be the calibration set for the paper\n",
    "\n",
    "train_data = data[:n]\n",
    "val_data = data[n:n1]\n",
    "calibrate_data = data[n1:]\n",
    "\n",
    "train_loader = BatchLoader(train_data, block_size, batch_size, device, name=\"train\")\n",
    "val_loader = BatchLoader(val_data, block_size, batch_size, device, name=\"val\")\n",
    "calibration_loader = BatchLoader(calibrate_data, block_size, batch_size, device, name=\"calibrate\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = GPT(vocab_size, block_size, n_embd, n_head, n_blocks, device, dropout)\n",
    "# model.to(device)\n",
    "\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# training_losses = train_loop(model, optimizer, vocab_size, train_loader, [train_loader, val_loader], max_iters, eval_interval, eval_iters)\n",
    "\n",
    "# print(\"training is done!\")\n",
    "\n",
    "# plt.title(\"training losses\")\n",
    "# plt.plot(training_losses)\n",
    "# plt.savefig(\"training_losses.png\")\n",
    "\n",
    "# idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "# print(tokenizer.decode(model.generate(idx, max_new_tokens=500)[0].tolist()))\n",
    "\n",
    "# model_params = {\n",
    "#     \"vocab_size\": vocab_size,\n",
    "#     \"block_size\": block_size,\n",
    "#     \"n_embd\": n_embd,\n",
    "#     \"n_head\": n_head,\n",
    "#     \"n_blocks\": n_blocks,\n",
    "#     \"dropout\": dropout,\n",
    "#     \"device\": device\n",
    "# }\n",
    "\n",
    "# save(model, tokenizer, model_params, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attn_head_importance_hook(module, ins, outs) -> None: # TODO: does the importance calculation returns the correct values for each head? \n",
    "    \"\"\" calculates the multi-head-attention layer's importance per head \"\"\"\n",
    "    # outs.shape = (B, T, E) where B: batch_size, T: num tokens, E: embedding size\n",
    "    # the importance is calculated as summing the L2 norm of the attn outputs on B and T dimensions\n",
    "    outs_flat = outs.view(-1, outs.shape[-1]) # (b,t,e) -> (b*t, e)\n",
    "    importance = torch.linalg.vector_norm(outs_flat.detach().cpu(), ord=2, dim=-1).sum()\n",
    "\n",
    "    module.calculated_importance = importance\n",
    "    \n",
    "    # print(outs_flat.shape)\n",
    "    # print(\"module:\", module.__class__.__name__, end=\" \")\n",
    "    # print(\"importance:\", importance)\n",
    "    # print(f\"{module.__class__.__name__} importance: {importance.shape}\")\n",
    "\n",
    "def neuron_importance_hook(module, ins, outs) -> None:\n",
    "    \"\"\" calculates the neuron importance for the given layer \"\"\" \n",
    "    \n",
    "    # the ffwd linear weights should be in the shape of (out, in)\n",
    "    # the paper sums up the values of (X * W_i^T) meaning (B, T, in) x (in, 1)= (B,T,1) -> (1, ) (summed up)\n",
    "    \n",
    "    # thus, in order to vectorize this operation, we'll need to hook this function to the first linear layer itself rather than the whole ffwd block. \n",
    "\n",
    "    # for each neuron in the ffwd layer, we can simply sum up the output columns\n",
    "\n",
    "    # as they're the activations of individual neurons\n",
    "    # calculate the importances\n",
    "    # importance = outs.detach().sum()\n",
    "    importance = outs.detach().cpu().sum(dim=(0,1))\n",
    "    # print(f\"{module.__class__.__name__} importance.shape: {importance.shape}\")\n",
    "\n",
    "    module.calculated_importance = importance\n",
    "\n",
    "def embedding_importance_hook(module, ins, outs) -> None:\n",
    "    # the first block's first processing layer will be the \n",
    "    # layer norm\n",
    "    # so we'll just sum up the layer norm outputs after getting them\n",
    "    # calculate the importances\n",
    "\n",
    "    importance = outs.detach().sum(dim=(0,1))\n",
    "    # print(\"importance.shape:\", importance.shape)\n",
    "    # print(\"n_embd: \", outs.size(-1))\n",
    "    # print(\"module:\", module.__class__.__name__)\n",
    "    # print(\"outs.shape:\", outs.shape) # probably (B, T, E)\n",
    " \n",
    "    module.calculated_importance = importance\n",
    "\n",
    "    # print(f\"{module.__class__.__name__} importance.shape: {importance.shape}\")\n",
    "\n",
    "def block_importance_hook(module, ins, outs) -> None:\n",
    "   \n",
    "    in_vectors = ins[0].detach()  # (B, T, E)\n",
    "    out_vectors = outs.detach()   # (B, T, E)\n",
    "    \n",
    "    # Calculate cosine similarity for each sample and time step\n",
    "    dot_product = torch.sum(in_vectors * out_vectors, dim=-1)  # (B, T)\n",
    "    in_norm = torch.norm(in_vectors, p=2, dim=-1)  # (B, T)\n",
    "    out_norm = torch.norm(out_vectors, p=2, dim=-1)  # (B, T)\n",
    "    \n",
    "    cosine_sim = dot_product / (in_norm * out_norm + 1e-8)  # (B, T)\n",
    "    \n",
    "    # Calculate BI by taking the expectation (mean) and subtracting from 1\n",
    "    block_importance = 1 - torch.mean(cosine_sim)\n",
    "    \n",
    "    # print(\"Block Importance:\", block_importance.item())\n",
    "    # print(\"module:\", module.__class__.__name__)\n",
    "    # print(\"outs.shape:\", outs.shape)  # (B, T, E)\n",
    " \n",
    "    module.calculated_importance = block_importance\n",
    "\n",
    "    # print(f\"{module.__class__.__name__} importance.shape: {block_importance.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the initial hooks for all the corresponding layers\n",
    "from models import Block, GPT\n",
    "\n",
    "def delete_importance_attr(layer: nn.Module):\n",
    "    if hasattr(layer, \"calculated_importance\"):\n",
    "        del layer.calculated_importance\n",
    "\n",
    "def remove_all_forward_hooks(model: GPT):\n",
    "    if not isinstance(model, GPT):\n",
    "        raise NotImplementedError(\"Only GPT models are supported for now\")\n",
    "    \n",
    "    for module in model.modules():\n",
    "        if isinstance(module, Block):\n",
    "            for head in module.sa.heads:\n",
    "                head._forward_hooks.clear()\n",
    "\n",
    "                head.key._forward_hooks.clear()\n",
    "                head.value._forward_hooks.clear()\n",
    "                head.query._forward_hooks.clear()\n",
    "\n",
    "                delete_importance_attr(head)\n",
    "                \n",
    "                delete_importance_attr(head.key)\n",
    "                delete_importance_attr(head.query)\n",
    "                delete_importance_attr(head.value)\n",
    "\n",
    "            module.ffwd.net[0]._forward_hooks.clear()\n",
    "            module.ln1._forward_hooks.clear()\n",
    "            module.sa._forward_hooks.clear()\n",
    "            module.sa.proj._forward_hooks.clear()\n",
    "            delete_importance_attr(module.ffwd.net[0])\n",
    "            delete_importance_attr(module.ln1)\n",
    "            delete_importance_attr(module.sa)\n",
    "            delete_importance_attr(module.sa.proj)\n",
    "\n",
    "def register_all_forward_hooks(model: GPT):\n",
    "    if not isinstance(model, GPT):\n",
    "        raise NotImplementedError(\"Only GPT models are supported for now\")\n",
    "\n",
    "    num_blocks = 0\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, Block):\n",
    "            num_blocks += 1\n",
    "            for head in module.sa.heads:\n",
    "                head.register_forward_hook(attn_head_importance_hook)\n",
    "\n",
    "                head.key.register_forward_hook(neuron_importance_hook)\n",
    "                head.value.register_forward_hook(neuron_importance_hook) \n",
    "                head.query.register_forward_hook(neuron_importance_hook)\n",
    "\n",
    "            module.ffwd.net[0].register_forward_hook(neuron_importance_hook) # register the forward hook to the linear layer inside of the ffwd block\n",
    "            module.sa.proj.register_forward_hook(neuron_importance_hook)\n",
    "            module.ln1.register_forward_hook(embedding_importance_hook)\n",
    "            module.register_forward_hook(block_importance_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinit_models():\n",
    "    loaded_model, tokenizer = load(GPT, \"model\")\n",
    "    loaded_model.to(device);\n",
    "\n",
    "    remove_all_forward_hooks(loaded_model)\n",
    "    register_all_forward_hooks(loaded_model)\n",
    "\n",
    "    return loaded_model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model, tokenizer = reinit_models()\n",
    "    s = 0\n",
    "    for k in model.parameters():\n",
    "        if k.requires_grad:\n",
    "            s += k.numel()\n",
    "    print(\"# trainable parameters:\", s)\n",
    "\n",
    "    sample_batch = calibrate_data[:batch_size*block_size]\n",
    "    sample_batch = sample_batch.view(batch_size, block_size)\n",
    "    sample_batch = sample_batch.to(device)\n",
    "\n",
    "    model(sample_batch);\n",
    "\n",
    "    return model, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neuron and head pruning? \n",
    "\n",
    "# start with neuron pruning\n",
    "\n",
    "def prune_neurons(model, ratio=0.2) -> None:\n",
    "    # goal: trim the MLP layer weights\n",
    "    # 1 - argsort the importances of the `ffwd` layers defined in the model\n",
    "    # 2 - remove the weights with respect to the given ratio\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, Block):\n",
    "            importances = module.ffwd.net[0].calculated_importance \n",
    "            num_neurons = int((1-ratio) * importances.size(0))\n",
    "            idx = importances.argsort(descending=True)[:num_neurons]\n",
    "            # reinitialize the weights along with the layer\n",
    "            dense1 = module.ffwd.net[0]\n",
    "            dense2 = module.ffwd.net[2]\n",
    "\n",
    "            module.ffwd.net[0] = nn.Linear(dense1.in_features, num_neurons).to(model.device) # weights.shape = (num_neurons, dense1.in_features)\n",
    "            module.ffwd.net[2] = nn.Linear(num_neurons, dense2.out_features).to(model.device) # weights.shape = (dense2.out_features = emb)\n",
    "\n",
    "            # now we need to set the weights to the new layers.\n",
    "\n",
    "            dense1.weight.data = dense1.weight.data[idx, :]\n",
    "            dense1.bias.data = dense1.bias.data[idx]\n",
    "\n",
    "            dense2.weight.data = dense2.weight.data[idx, :]\n",
    "            dense2.bias.data = dense2.bias.data[idx]\n",
    "\n",
    "            module.ffwd.net[0].calculated_importance = importances[idx]\n",
    "            module.ffwd.net[2].calculated_importance = importances[idx]\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def prune_heads(model, ratio=0.2) -> None:\n",
    "    # goal: trim the attention heads' layer weights using the same approach as the `prune_neurons`\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, Block):\n",
    "            # now the multi-head attention\n",
    "            for head in module.sa.heads:\n",
    "                # key,value,query weight shape: (head_size, n_embd) # n_embd\n",
    "                k,v,q = head.key, head.value, head.query\n",
    "\n",
    "                key_importances = head.key.calculated_importance\n",
    "                value_importances = head.value.calculated_importance\n",
    "                query_importances = head.query.calculated_importance\n",
    "\n",
    "                num_neurons = int((1-ratio) * key_importances.size(0))\n",
    "\n",
    "\n",
    "                k_idx = key_importances.argsort(descending=True)[:num_neurons]\n",
    "                v_idx = value_importances.argsort(descending=True)[:num_neurons]\n",
    "                q_idx = query_importances.argsort(descending=True)[:num_neurons]\n",
    "\n",
    "                head.key = nn.Linear(k.in_features, num_neurons, bias=False).to(model.device) \n",
    "                head.value = nn.Linear(v.in_features, num_neurons, bias=False).to(model.device) \n",
    "                head.query = nn.Linear(q.in_features, num_neurons, bias=False).to(model.device) \n",
    "\n",
    "                head.key.weight.data = k.weight.data[k_idx, :] # (head_size, num_dense_embd)\n",
    "                head.value.weight.data = v.weight.data[v_idx, :] # (head_size, num_dense_embd)\n",
    "                head.query.weight.data = q.weight.data[q_idx, :] # (head_size, num_dense_embd)\n",
    "\n",
    "                head.key.calculated_importance = key_importances[k_idx]\n",
    "                head.value.calculated_importance = value_importances[v_idx]\n",
    "                head.query.calculated_importance = query_importances[q_idx]\n",
    "\n",
    "                # TODO: only the weights in the embedding layers are prunned (1st strategy)\n",
    "                # TODO: need to follow the correct implementation from the paper (pruning every linear layer?)\n",
    "\n",
    "            proj = module.sa.proj\n",
    "            proj_importances = module.sa.proj.calculated_importance\n",
    "            num_neurons = int((1-ratio) * key_importances.size(0)) * module.sa.num_heads\n",
    "            idx = proj_importances.argsort(descending=True)[:num_neurons]\n",
    "\n",
    "            module.sa.proj = nn.Linear(num_neurons, proj.out_features).to(model.device)\n",
    "\n",
    "            module.sa.proj.weight.data = proj.weight.data[:, idx]\n",
    "            module.sa.proj.bias.data = proj.bias.data\n",
    "\n",
    "            module.sa.proj.calculated_importance = proj_importances[idx]\n",
    "\n",
    "def prune_embeddings(model, ratio=0.2) -> None:\n",
    "    # goal: trim the embedding dimension of the weight matrices in MLP, MHA, and LayerNorm layers.\n",
    "    # TODO: check how embedding importance is calculated!\n",
    "    \n",
    "    for module in model.modules():\n",
    "        if isinstance(module, Block):\n",
    "            # start with pruning the MLP layers\n",
    "            importances = module.ln1.calculated_importance\n",
    "\n",
    "            dense1 = module.ffwd.net[0] # weights.shape = (emb, 4 * emb)\n",
    "            dense2 = module.ffwd.net[2] # weights.shape = (4 * emb, emb)\n",
    "\n",
    "            num_dense_embd = int((1-ratio) * dense1.in_features)\n",
    "            idx = importances.argsort(descending=True)[:num_dense_embd]\n",
    "\n",
    "            module.ffwd.net[0] = nn.Linear(num_dense_embd, dense1.out_features).to(model.device) # weights.shape = (num_dense_embd, dense1.in_features)\n",
    "            module.ffwd.net[2] = nn.Linear(dense2.in_features, num_dense_embd).to(model.device) # weights.shape = (dense2.out_features = emb)\n",
    "\n",
    "            module.ffwd.net[0].weight.data = dense1.weight.data[:, idx]\n",
    "            module.ffwd.net[0].bias.data = dense1.bias.data\n",
    "            module.ffwd.net[2].weight.data = dense2.weight.data[idx, :]\n",
    "            module.ffwd.net[2].bias.data = dense2.bias.data[idx]        \n",
    "\n",
    "\n",
    "            # now the multi-head attention\n",
    "            for head in module.sa.heads:\n",
    "                # key,value,query weight shape: (head_size, n_embd) # n_embd\n",
    "                k,v,q = head.key, head.value, head.query\n",
    "                  \n",
    "                head.key = nn.Linear(num_dense_embd, k.out_features, bias=False).to(model.device) \n",
    "                head.value = nn.Linear(num_dense_embd, v.out_features, bias=False).to(model.device) \n",
    "                head.query = nn.Linear(num_dense_embd, q.out_features, bias=False).to(model.device) \n",
    "\n",
    "                head.key.weight.data = k.weight.data[:, idx] # (head_size, num_dense_embd)\n",
    "                head.value.weight.data = v.weight.data[:, idx] # (head_size, num_dense_embd)\n",
    "                head.query.weight.data = q.weight.data[:, idx] # (head_size, num_dense_embd)\n",
    "\n",
    "                head.key.calculated_importance = k.calculated_importance\n",
    "                head.value.calculated_importance = v.calculated_importance \n",
    "                head.query.calculated_importance = q.calculated_importance\n",
    "\n",
    "            ln1 = module.ln1\n",
    "            ln2 = module.ln2\n",
    "\n",
    "            module.ln1 = nn.LayerNorm(num_dense_embd).to(model.device) \n",
    "            module.ln1.weight.data = ln1.weight.data[idx]\n",
    "            module.ln1.bias.data = ln1.bias.data[idx]\n",
    "\n",
    "            module.ln2 = nn.LayerNorm(num_dense_embd).to(model.device) \n",
    "            module.ln2.weight.data = ln2.weight.data[idx]\n",
    "            module.ln2.bias.data = ln2.bias.data[idx]\n",
    "\n",
    "            proj = module.sa.proj\n",
    "            module.sa.proj = nn.Linear(proj.in_features, num_dense_embd).to(model.device) \n",
    "            module.sa.proj.weight.data = proj.weight.data[idx, :] # (num_dense_embd, n_embd)\n",
    "            module.sa.proj.bias.data = proj.bias.data[idx]\n",
    "            \n",
    "            module.sa.proj.calculated_importance = proj.calculated_importance\n",
    "\n",
    "    \n",
    "    temb_table = model.token_embedding_table\n",
    "    pemb_table = model.position_embedding_table\n",
    "\n",
    "    model.token_embedding_table = nn.Embedding(vocab_size, num_dense_embd).to(device)\n",
    "    model.position_embedding_table = nn.Embedding(model.block_size, num_dense_embd).to(device)\n",
    "\n",
    "    model.token_embedding_table.weight.data = temb_table.weight.data[:, idx]\n",
    "    model.position_embedding_table.weight.data = pemb_table.weight.data[:, idx]\n",
    "\n",
    "    lnf = model.ln_f\n",
    "    ln_head = model.ln_head\n",
    "\n",
    "    model.ln_f = nn.LayerNorm(num_dense_embd).to(device)\n",
    "    model.ln_head = nn.Linear(num_dense_embd, ln_head.out_features).to(device) \n",
    "\n",
    "    model.ln_f.weight.data = lnf.weight.data[idx]\n",
    "    model.ln_f.bias.data = lnf.bias.data[idx]\n",
    "    model.ln_head.weight.data = ln_head.weight.data[:, idx] # weight.shape = (vocab_size, embd)\n",
    "    model.ln_head.bias.data = ln_head.bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alperiox/Desktop/coding/paper-implts/Compact_Language_Models_240714679/utils.py:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(save_dir / \"model.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# trainable parameters: 3222593\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'val': tensor(2.0092)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, s = get_model()\n",
    "\n",
    "base_loss = estimate_loss(model, val_loader)\n",
    "base_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_heads(model, 0.1);\n",
    "prune_neurons(model, 0.1);\n",
    "prune_embeddings(model, 0.1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (token_embedding_table): Embedding(65, 230)\n",
       "  (position_embedding_table): Embedding(128, 230)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (sa): MultiHeadAttentionConcat(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=230, out_features=57, bias=False)\n",
       "            (query): Linear(in_features=230, out_features=57, bias=False)\n",
       "            (value): Linear(in_features=230, out_features=57, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=228, out_features=230, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=230, out_features=921, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=921, out_features=230, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((230,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((230,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (sa): MultiHeadAttentionConcat(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=230, out_features=57, bias=False)\n",
       "            (query): Linear(in_features=230, out_features=57, bias=False)\n",
       "            (value): Linear(in_features=230, out_features=57, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=228, out_features=230, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=230, out_features=921, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=921, out_features=230, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((230,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((230,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (sa): MultiHeadAttentionConcat(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=230, out_features=57, bias=False)\n",
       "            (query): Linear(in_features=230, out_features=57, bias=False)\n",
       "            (value): Linear(in_features=230, out_features=57, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=228, out_features=230, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=230, out_features=921, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=921, out_features=230, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((230,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((230,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (sa): MultiHeadAttentionConcat(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=230, out_features=57, bias=False)\n",
       "            (query): Linear(in_features=230, out_features=57, bias=False)\n",
       "            (value): Linear(in_features=230, out_features=57, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=228, out_features=230, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=230, out_features=921, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=921, out_features=230, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((230,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((230,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((230,), eps=1e-05, elementwise_affine=True)\n",
       "  (ln_head): Linear(in_features=230, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_params(model):\n",
    "    t = 0\n",
    "    for k in model.parameters():\n",
    "        if k.requires_grad:\n",
    "            t += k.numel()\n",
    "    \n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2602749\n",
      "total difference:  619844\n",
      "diff ratio:  0.19234324657193758\n"
     ]
    }
   ],
   "source": [
    "t = 0\n",
    "for k in model.parameters():\n",
    "    if k.requires_grad:\n",
    "        t += k.numel()\n",
    "print(t)\n",
    "\n",
    "print(\"total difference: \", s-t)\n",
    "print(\"diff ratio: \", (s-t)/s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val': tensor(3.8883)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before_calibration = estimate_loss(model, val_loader)\n",
    "before_calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05555588760915432"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calibrate_data.shape[0] / train_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIFORM BASELINE:  4.174387454986572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "step 150: calibrate loss 2.2990, val loss 2.3771,  \t | baseline (uniform random): 4.1744: 100%|██████████| 200/200 [00:36<00:00,  5.50it/s]\n"
     ]
    }
   ],
   "source": [
    "losses = train_loop(model, optimizer, vocab_size, calibration_loader, [calibration_loader, val_loader], max_iters = 200, eval_interval=50, eval_iters=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val': tensor(2.2064)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "after_calibration = estimate_loss(model, val_loader)\n",
    "after_calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base                : 2.0092\n",
      "before calibration  : 3.8883\n",
      "after calibration   : 2.2064\n"
     ]
    }
   ],
   "source": [
    "losses = [base_loss, before_calibration, after_calibration]\n",
    "names = [\"base\", \"before calibration\", \"after calibration\"]\n",
    "for n, l in zip(names, losses):\n",
    "    print(f\"{n:20}: {l['val']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies = {\n",
    "    \"width_head\": prune_heads,\n",
    "    \"width_neuron\": prune_neurons,\n",
    "    \"width_embedding\": prune_embeddings\n",
    "}\n",
    "\n",
    "def experiment(pruning_strategies: list[list[str]] = [[(\"width_head\", 0.1), (\"width_neuron\", 0.1), (\"width_embedding\", 0.1)]], learning_rate: float=2e-3):\n",
    "    # repeat len(pruning_ratio) times\n",
    "\n",
    "        # model initialize\n",
    "\n",
    "        # model pruning\n",
    "\n",
    "        # pruned eval\n",
    "\n",
    "        # calibration\n",
    "\n",
    "        # calibration eval\n",
    "\n",
    "    results = []\n",
    "\n",
    "    model, num_params = get_model()\n",
    "    base_loss = estimate_loss(model, val_loader)['val'].item()\n",
    "\n",
    "    for run in range(len(pruning_strategies)):\n",
    "        print(\"-\"*50)\n",
    "        strategy = pruning_strategies[run]\n",
    "\n",
    "        pruning_funcs = [strategies[s] for s, ratio in strategy]\n",
    "        pruning_func_names = [s for s, ratio in strategy]\n",
    "        ratios = [ratio for s, ratio in strategy]\n",
    "\n",
    "        print(f\"RUN {run+1} | RATIO: {ratios} | STRATEGIES: {pruning_func_names}\")\n",
    "        model, num_params = get_model()\n",
    "        print(f\"{'Number of trainable parameters before pruning:':60}\", num_params)\n",
    "        # prune\n",
    "        for f, r in zip(pruning_funcs, ratios):\n",
    "            f(model, r)\n",
    "        #\n",
    "        pruned_num_params = get_num_params(model)\n",
    "        param_diff_ratio = ((num_params-pruned_num_params)/num_params)\n",
    "        print(f\"{'Number of training parameters after pruning:':60} {pruned_num_params}\")\n",
    "        print(f\"{'Ratio of the pruned weights to the base model:':60} {param_diff_ratio*100:.2f}%\")\n",
    "        pruned_eval = estimate_loss(model, val_loader)['val'].item()\n",
    "        print(f\"{'Pruned evaluation loss (before calibration):':60} {pruned_eval:.4f}\")\n",
    "        #\n",
    "        print(\"Starting the calibration\")\n",
    "        optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "        losses = train_loop(model, optimizer, vocab_size, calibration_loader, [calibration_loader, val_loader], max_iters = 200, eval_interval=50, eval_iters=50)\n",
    "        #\n",
    "        calibrated_eval = estimate_loss(model, val_loader)['val'].item()\n",
    "        print(f\"{'Pruned evaluation loss (after calibration):':60} {calibrated_eval:.4f}\")\n",
    "\n",
    "        \n",
    "        result = {\n",
    "            \"run\": run+1,\n",
    "            \"base_num_params\": num_params,\n",
    "            \"pruned_num_params\": pruned_num_params,\n",
    "            \"pruning_ratio\": ratios,\n",
    "            \"param_diff_ratio\": param_diff_ratio,\n",
    "            \"before_calibration_loss\": pruned_eval,\n",
    "            \"after_calibration_loss\": calibrated_eval,\n",
    "            \"base_loss\": base_loss,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"pruning_strategies\": pruning_func_names,\n",
    "            \"training_losses\": losses\n",
    "        }\n",
    "\n",
    "\n",
    "        results.append(result)\n",
    "        run_df = pd.DataFrame(results)\n",
    "        run_df.to_csv(f\"run_results.csv\", index=False)\n",
    "    \n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alperiox/Desktop/coding/paper-implts/Compact_Language_Models_240714679/utils.py:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(save_dir / \"model.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# trainable parameters: 3222593\n",
      "--------------------------------------------------\n",
      "RUN 1 | RATIO: [0.1, 0.1, 0.1] | STRATEGIES: ['width_head', 'width_neuron', 'width_embedding']\n",
      "# trainable parameters: 3222593\n",
      "Number of trainable parameters before pruning:               3222593\n",
      "Number of training parameters after pruning:                 2602749\n",
      "Ratio of the pruned weights to the base model:               19.23%\n",
      "Pruned evaluation loss (before calibration):                 4.1108\n",
      "Starting the calibration\n",
      "UNIFORM BASELINE:  4.174387454986572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "step 150: calibrate loss 2.1932, val loss 2.2591,  \t | baseline (uniform random): 4.1744: 100%|██████████| 200/200 [00:56<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned evaluation loss (after calibration):                  2.1262\n",
      "Base to Pruned loss ratio:                                   -1.0367\n",
      "Pruned to Calibrated loss ratio:                             0.4828\n",
      "Base to Calibrated loss ratio:                               -0.0534\n",
      "--------------------------------------------------\n",
      "RUN 2 | RATIO: [0.2, 0.2, 0.2] | STRATEGIES: ['width_head', 'width_neuron', 'width_embedding']\n",
      "# trainable parameters: 3222593\n",
      "Number of trainable parameters before pruning:               3222593\n",
      "Number of training parameters after pruning:                 2063741\n",
      "Ratio of the pruned weights to the base model:               35.96%\n",
      "Pruned evaluation loss (before calibration):                 4.0677\n",
      "Starting the calibration\n",
      "UNIFORM BASELINE:  4.174387454986572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "step 150: calibrate loss 2.2209, val loss 2.2831,  \t | baseline (uniform random): 4.1744: 100%|██████████| 200/200 [00:40<00:00,  4.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned evaluation loss (after calibration):                  2.1476\n",
      "Base to Pruned loss ratio:                                   -1.0153\n",
      "Pruned to Calibrated loss ratio:                             0.4720\n",
      "Base to Calibrated loss ratio:                               -0.0640\n"
     ]
    }
   ],
   "source": [
    "experiment_config = [\n",
    "    [(\"width_head\", 0.1), (\"width_neuron\", 0.1), (\"width_embedding\", 0.1)],\n",
    "    [(\"width_head\", 0.2), (\"width_neuron\", 0.2), (\"width_embedding\", 0.2)],\n",
    "]\n",
    "\n",
    "exp_results = experiment(pruning_strategies=experiment_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run</th>\n",
       "      <th>base_num_params</th>\n",
       "      <th>pruned_num_params</th>\n",
       "      <th>pruning_ratio</th>\n",
       "      <th>param_diff_ratio</th>\n",
       "      <th>before_calibration_loss</th>\n",
       "      <th>after_calibration_loss</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>pruning_strategies</th>\n",
       "      <th>training_losses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3222593</td>\n",
       "      <td>2602749</td>\n",
       "      <td>[0.1, 0.1, 0.1]</td>\n",
       "      <td>0.192343</td>\n",
       "      <td>tensor(4.0054)</td>\n",
       "      <td>tensor(2.1326)</td>\n",
       "      <td>0.002</td>\n",
       "      <td>[width_head, width_neuron, width_embedding]</td>\n",
       "      <td>[0.6039924025535583, 0.5813836455345154, 0.525...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3222593</td>\n",
       "      <td>2063741</td>\n",
       "      <td>[0.2, 0.2, 0.2]</td>\n",
       "      <td>0.359602</td>\n",
       "      <td>tensor(3.9960)</td>\n",
       "      <td>tensor(2.1720)</td>\n",
       "      <td>0.002</td>\n",
       "      <td>[width_head, width_neuron, width_embedding]</td>\n",
       "      <td>[0.5998151898384094, 0.5458047986030579, 0.515...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   run  base_num_params  pruned_num_params    pruning_ratio  param_diff_ratio  \\\n",
       "0    1          3222593            2602749  [0.1, 0.1, 0.1]          0.192343   \n",
       "1    2          3222593            2063741  [0.2, 0.2, 0.2]          0.359602   \n",
       "\n",
       "  before_calibration_loss after_calibration_loss  learning_rate  \\\n",
       "0          tensor(4.0054)         tensor(2.1326)          0.002   \n",
       "1          tensor(3.9960)         tensor(2.1720)          0.002   \n",
       "\n",
       "                            pruning_strategies  \\\n",
       "0  [width_head, width_neuron, width_embedding]   \n",
       "1  [width_head, width_neuron, width_embedding]   \n",
       "\n",
       "                                     training_losses  \n",
       "0  [0.6039924025535583, 0.5813836455345154, 0.525...  \n",
       "1  [0.5998151898384094, 0.5458047986030579, 0.515...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.DataFrame(exp_results)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "y = model(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cean cablikntees\n",
      "A-:\n",
      "Hakn Iblast tound om ninceined sling?\n",
      "My wend he prorr:\n",
      "Mod mig,\n",
      "s ie, w who sithe suby, st, thid hand shin ght. bud pond t, htes, ourthan.\n",
      "A st my when's ud PETIAnd y, d, polad l ad, hivand mses. y?\n",
      "ano,\n",
      "\n",
      "\n",
      "PPRO:\n",
      "PEThyo'd to my, w, se syou lyand sshay t ye ned hak's peakearnst je. s,\n",
      "And wn. SI nd s le My atith wofa de an,-fr she tharof thak By sswa mo mo De bu, GRe, d. ANDhise waou nt histbe, os y tow mirho se; ba nenerer nir I in byhealin s re fo y cat whethe s inge y nd.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(tokenizer.decode(model.generate(idx, max_new_tokens=500)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
