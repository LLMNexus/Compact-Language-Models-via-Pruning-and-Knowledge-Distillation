{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models import GPT\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "from utils import BatchLoader, estimate_loss, train_loop\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16  # number of independent sequences that'll be processed in parallel\n",
    "block_size = 128  # maximum context length for the preds\n",
    "max_iters = 1000\n",
    "eval_interval = 200\n",
    "learning_rate = 3e-4\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "eval_iters = 200\n",
    "n_embd = 256\n",
    "n_head = 4\n",
    "n_blocks = 4\n",
    "dropout = 0.2\n",
    "# --------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# data preparation\n",
    "text = open(\"dataset/tinyshakespeare.txt\", \"r\").read()\n",
    "# set up the vocabulary\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "tokenizer = Tokenizer(chars)\n",
    "\n",
    "data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "\n",
    "n = int(0.9 * len(data))  # first 90% will be the training set\n",
    "n1 = int(0.98 * len(data))  # 90-98% will be the validation set and the last 2% will be the calibration set for the paper\n",
    "\n",
    "train_data = data[:n]\n",
    "val_data = data[n:n1]\n",
    "calibrate_data = data[n1:]\n",
    "\n",
    "train_loader = BatchLoader(train_data, block_size, batch_size, device, name=\"train\")\n",
    "val_loader = BatchLoader(val_data, block_size, batch_size, device, name=\"val\")\n",
    "calibration_loader = BatchLoader(calibrate_data, block_size, batch_size, device, name=\"calibrate\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alperiox/Desktop/coding/paper-implts/Compact_Language_Models_240714679/utils.py:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(save_dir / \"model.pth\"))\n"
     ]
    }
   ],
   "source": [
    "from utils import load\n",
    "\n",
    "loaded_model, tokenizer = load(GPT, \"model\")\n",
    "loaded_model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attn_head_importance_hook(module, ins, outs) -> None: # TODO: does the importance calculation returns the correct values for each head? \n",
    "    \"\"\" calculates the multi-head-attention layer's importance per head \"\"\"\n",
    "\n",
    "    # outs.shape = (B, T, E) where B: batch_size, T: num tokens, E: embedding size\n",
    "    # the importance is calculated as summing the L2 norm of the attn outputs on B and T dimensions\n",
    "    outs_flat = outs.view(-1, outs.shape[-1]) # (b,t,e) -> (b*t, e)\n",
    "    importance = torch.linalg.vector_norm(outs_flat.detach().cpu(), ord=2, dim=-1).sum()\n",
    "\n",
    "    module.calculated_importance = importance\n",
    "    \n",
    "    # print(outs_flat.shape)\n",
    "    # print(\"module:\", module.__class__.__name__, end=\" \")\n",
    "    # print(\"importance:\", importance)\n",
    "    # print(f\"{module.__class__.__name__} importance: {importance.shape}\")\n",
    "\n",
    "def neuron_importance_hook(module, ins, outs) -> None:\n",
    "    \"\"\" calculates the neuron importance for the given layer \"\"\" \n",
    "    \n",
    "    # the ffwd linear weights should be in the shape of (out, in)\n",
    "    # the paper sums up the values of (X * W_i^T) meaning (B, T, in) x (in, 1)= (B,T,1) -> (1, ) (summed up)\n",
    "    \n",
    "    # thus, in order to vectorize this operation, we'll need to hook this function to the first linear layer itself rather than the whole ffwd block. \n",
    "\n",
    "    # for each neuron in the ffwd layer, we can simply sum up the output columns\n",
    "\n",
    "    # as they're the activations of individual neurons\n",
    "    # calculate the importances\n",
    "    # importance = outs.detach().sum()\n",
    "    importance = outs.detach().cpu().sum(dim=(0,1))\n",
    "    # print(f\"{module.__class__.__name__} importance.shape: {importance.shape}\")\n",
    "\n",
    "    module.calculated_importance = importance\n",
    "\n",
    "def embedding_importance_hook(module, ins, outs) -> None:\n",
    "    # the first block's first processing layer will be the \n",
    "    # layer norm,\n",
    "    # so we'll just sum up the layer norm outputs after getting them\n",
    "   # calculate the importances\n",
    "    importance = outs.detach().sum(dim=(0,1))\n",
    "    # print(\"importance.shape:\", importance.shape)\n",
    "    # print(\"n_embd: \", outs.size(-1))\n",
    "    # print(\"module:\", module.__class__.__name__)\n",
    "    # print(\"outs.shape:\", outs.shape) # probably (B, T, E)\n",
    " \n",
    "    module.calculated_importance = importance\n",
    "\n",
    "    # print(f\"{module.__class__.__name__} importance.shape: {importance.shape}\")\n",
    "\n",
    "def block_importance_hook(module, ins, outs) -> None:\n",
    "   \n",
    "    in_vectors = ins[0].detach()  # (B, T, E)\n",
    "    out_vectors = outs.detach()   # (B, T, E)\n",
    "    \n",
    "    # Calculate cosine similarity for each sample and time step\n",
    "    dot_product = torch.sum(in_vectors * out_vectors, dim=-1)  # (B, T)\n",
    "    in_norm = torch.norm(in_vectors, p=2, dim=-1)  # (B, T)\n",
    "    out_norm = torch.norm(out_vectors, p=2, dim=-1)  # (B, T)\n",
    "    \n",
    "    cosine_sim = dot_product / (in_norm * out_norm + 1e-8)  # (B, T)\n",
    "    \n",
    "    # Calculate BI by taking the expectation (mean) and subtracting from 1\n",
    "    block_importance = 1 - torch.mean(cosine_sim)\n",
    "    \n",
    "    # print(\"Block Importance:\", block_importance.item())\n",
    "    # print(\"module:\", module.__class__.__name__)\n",
    "    # print(\"outs.shape:\", outs.shape)  # (B, T, E)\n",
    " \n",
    "    module.calculated_importance = block_importance\n",
    "\n",
    "    # print(f\"{module.__class__.__name__} importance.shape: {block_importance.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the initial hooks for all the corresponding layers\n",
    "from models import Block, GPT\n",
    "\n",
    "def delete_importance_attr(layer: nn.Module):\n",
    "    if hasattr(layer, \"calculated_importance\"):\n",
    "        del layer.calculated_importance\n",
    "\n",
    "def remove_all_forward_hooks(model: GPT):\n",
    "    if not isinstance(model, GPT):\n",
    "        raise NotImplementedError(\"Only GPT models are supported for now\")\n",
    "    \n",
    "    for module in model.modules():\n",
    "        if isinstance(module, Block):\n",
    "            for head in module.sa.heads:\n",
    "                head._forward_hooks.clear()\n",
    "                delete_importance_attr(head)\n",
    "\n",
    "            module.ffwd.net[0]._forward_hooks.clear()\n",
    "            module.ln1._forward_hooks.clear()\n",
    "            module.sa._forward_hooks.clear()\n",
    "\n",
    "            delete_importance_attr(module.ffwd.net[0])\n",
    "            delete_importance_attr(module.ln1)\n",
    "            delete_importance_attr(module.sa)\n",
    "\n",
    "def register_all_forward_hooks(model: GPT):\n",
    "    if not isinstance(model, GPT):\n",
    "        raise NotImplementedError(\"Only GPT models are supported for now\")\n",
    "\n",
    "    num_blocks = 0\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, Block):\n",
    "            num_blocks += 1\n",
    "            for head in module.sa.heads:\n",
    "                head.register_forward_hook(attn_head_importance_hook)\n",
    "            module.ffwd.net[0].register_forward_hook(neuron_importance_hook) # register the forward hook to the linear layer inside of the ffwd block\n",
    "            if num_blocks == 1:\n",
    "                module.ln1.register_forward_hook(embedding_importance_hook)\n",
    "            module.register_forward_hook(block_importance_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinit_models():\n",
    "    loaded_model, tokenizer = load(GPT, \"model\")\n",
    "    loaded_model.to(device);\n",
    "\n",
    "    remove_all_forward_hooks(loaded_model)\n",
    "    register_all_forward_hooks(loaded_model)\n",
    "\n",
    "    return loaded_model, tokenizer\n",
    "\n",
    "model, tokenizer = reinit_models()\n",
    "\n",
    "# estimate_loss(model, val_loader) # 2.0079"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3222593\n"
     ]
    }
   ],
   "source": [
    "s = 0\n",
    "for k in model.parameters():\n",
    "    if k.requires_grad:\n",
    "        s += k.numel()\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ixs = torch.randint(0, calibrate_data.size(0), (batch_size*block_size, ))\n",
    "\n",
    "sample_batch = calibrate_data[ixs]\n",
    "sample_batch = sample_batch.view(batch_size, block_size)\n",
    "sample_batch = sample_batch.to(device)\n",
    "\n",
    "model(sample_batch);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neuron and head pruning? \n",
    "\n",
    "# start with neuron pruning\n",
    "\n",
    "def prune_neurons(model, ratio=0.2) -> None:\n",
    "    # goal: trim the MLP layer weights\n",
    "    # 1 - argsort the importances of the `ffwd` layers defined in the model\n",
    "    # 2 - remove the weights with respect to the given ratio\n",
    "\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, Block):\n",
    "            importances = module.ffwd.net[0].calculated_importance \n",
    "            num_neurons = int((1-ratio) * importances.size(0))\n",
    "            idx = importances.argsort(descending=True)[:num_neurons]\n",
    "            # reinitialize the weights along with the layer\n",
    "            dense1 = module.ffwd.net[0]\n",
    "            dense2 = module.ffwd.net[2]\n",
    "\n",
    "            module.ffwd.net[0] = nn.Linear(dense1.in_features, num_neurons).to(model.device) # weights.shape = (num_neurons, dense1.in_features)\n",
    "            module.ffwd.net[2] = nn.Linear(num_neurons, dense2.out_features).to(model.device) # weights.shape = (dense2.out_features = emb)\n",
    "\n",
    "            # now we need to set the weights to the new layers.\n",
    "\n",
    "            dense1.weight.data = dense1.weight.data[idx, :]\n",
    "            dense1.bias.data = dense1.bias.data[idx]\n",
    "\n",
    "            dense2.weight.data = dense2.weight.data[idx, :]\n",
    "            dense2.bias.data = dense2.bias.data[idx]\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def prune_heads(model, ratio=0.2) -> None:\n",
    "    # goal: trim the attention heads' layer weights using the same approach as the `prune_neurons`\n",
    "    pass\n",
    "\n",
    "\n",
    "def prune_embeddings(model, ratio=0.2) -> None:\n",
    "    # goal: trim the embedding dimension of the weight matrices in MLP, MHA, and LayerNorm layers.\n",
    "    # TODO: check how embedding importance is calculated!\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (token_embedding_table): Embedding(65, 256)\n",
       "  (position_embedding_table): Embedding(128, 256)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (sa): MultiHeadAttentionConcat(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=256, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=256, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=256, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=819, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=819, out_features=256, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (sa): MultiHeadAttentionConcat(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=256, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=256, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=256, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=819, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=819, out_features=256, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (sa): MultiHeadAttentionConcat(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=256, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=256, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=256, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=819, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=819, out_features=256, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (sa): MultiHeadAttentionConcat(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=256, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=256, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=256, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=819, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=819, out_features=256, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (ln_head): Linear(in_features=256, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prune_neurons(model, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2801933\n",
      "total difference:  420660\n",
      "diff ratio:  0.1305346346870362\n"
     ]
    }
   ],
   "source": [
    "t = 0\n",
    "for k in model.parameters():\n",
    "    if k.requires_grad:\n",
    "        t += k.numel()\n",
    "print(t)\n",
    "\n",
    "print(\"total difference: \", s-t)\n",
    "print(\"diff ratio: \", (s-t)/s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val': tensor(3.1024)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_loss(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIFORM BASELINE:  4.174387454986572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "step 150: calibrate loss 1.8349, val loss 2.1691,  \t | baseline (uniform random): 4.1744: 100%|██████████| 200/200 [01:58<00:00,  1.68it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5107743740081787,\n",
       " 0.4602510333061218,\n",
       " 0.4431624114513397,\n",
       " 0.4300287961959839,\n",
       " 0.42028582096099854,\n",
       " 0.4176865518093109,\n",
       " 0.4054933786392212,\n",
       " 0.3975889980792999,\n",
       " 0.4035460650920868,\n",
       " 0.3891201615333557,\n",
       " 0.380073606967926,\n",
       " 0.3809751868247986,\n",
       " 0.3830280900001526,\n",
       " 0.36982008814811707,\n",
       " 0.37740984559059143,\n",
       " 0.3718697130680084,\n",
       " 0.37604257464408875,\n",
       " 0.3682430684566498,\n",
       " 0.3606867790222168,\n",
       " 0.3529806137084961,\n",
       " 0.36129114031791687,\n",
       " 0.34820279479026794,\n",
       " 0.34358736872673035,\n",
       " 0.35382091999053955,\n",
       " 0.34895434975624084,\n",
       " 0.34823405742645264,\n",
       " 0.3469099700450897,\n",
       " 0.35234910249710083,\n",
       " 0.3467314541339874,\n",
       " 0.3486853241920471,\n",
       " 0.3428036570549011,\n",
       " 0.3350028097629547,\n",
       " 0.3481793999671936,\n",
       " 0.3514370918273926,\n",
       " 0.3434023857116699,\n",
       " 0.32682523131370544,\n",
       " 0.3314991295337677,\n",
       " 0.31998324394226074,\n",
       " 0.3255039155483246,\n",
       " 0.32986509799957275,\n",
       " 0.33886486291885376,\n",
       " 0.3355449438095093,\n",
       " 0.32665982842445374,\n",
       " 0.3302578330039978,\n",
       " 0.31407833099365234,\n",
       " 0.31431102752685547,\n",
       " 0.3277814984321594,\n",
       " 0.33588147163391113,\n",
       " 0.3320920765399933,\n",
       " 0.3184000551700592,\n",
       " 0.3004482686519623,\n",
       " 0.31836965680122375,\n",
       " 0.32174983620643616,\n",
       " 0.3152157664299011,\n",
       " 0.3123634159564972,\n",
       " 0.31781336665153503,\n",
       " 0.3138386607170105,\n",
       " 0.3220641314983368,\n",
       " 0.30681943893432617,\n",
       " 0.30619102716445923,\n",
       " 0.30031344294548035,\n",
       " 0.32040396332740784,\n",
       " 0.316806823015213,\n",
       " 0.31641674041748047,\n",
       " 0.31602224707603455,\n",
       " 0.31364187598228455,\n",
       " 0.31531190872192383,\n",
       " 0.33128806948661804,\n",
       " 0.3131748139858246,\n",
       " 0.32968583703041077,\n",
       " 0.3135187029838562,\n",
       " 0.3064565062522888,\n",
       " 0.320622056722641,\n",
       " 0.3018004298210144,\n",
       " 0.3161943554878235,\n",
       " 0.2998429238796234,\n",
       " 0.30517399311065674,\n",
       " 0.31152403354644775,\n",
       " 0.3020075857639313,\n",
       " 0.31432002782821655,\n",
       " 0.309693843126297,\n",
       " 0.31257763504981995,\n",
       " 0.30933690071105957,\n",
       " 0.3211718797683716,\n",
       " 0.3188973665237427,\n",
       " 0.3031576871871948,\n",
       " 0.31151726841926575,\n",
       " 0.29716795682907104,\n",
       " 0.29314687848091125,\n",
       " 0.31000518798828125,\n",
       " 0.3038994371891022,\n",
       " 0.31218069791793823,\n",
       " 0.30063846707344055,\n",
       " 0.28794968128204346,\n",
       " 0.30823594331741333,\n",
       " 0.29613712430000305,\n",
       " 0.2928193509578705,\n",
       " 0.29960763454437256,\n",
       " 0.30066728591918945,\n",
       " 0.30414634943008423,\n",
       " 0.30807843804359436,\n",
       " 0.2960381805896759,\n",
       " 0.277773380279541,\n",
       " 0.2992594242095947,\n",
       " 0.2909770905971527,\n",
       " 0.29761800169944763,\n",
       " 0.2953954041004181,\n",
       " 0.30837640166282654,\n",
       " 0.30230632424354553,\n",
       " 0.30578240752220154,\n",
       " 0.28527769446372986,\n",
       " 0.3006982207298279,\n",
       " 0.29908186197280884,\n",
       " 0.29951152205467224,\n",
       " 0.29251018166542053,\n",
       " 0.30228912830352783,\n",
       " 0.2938574552536011,\n",
       " 0.2977505624294281,\n",
       " 0.3003413677215576,\n",
       " 0.2829650938510895,\n",
       " 0.3054312765598297,\n",
       " 0.2985895276069641,\n",
       " 0.295622855424881,\n",
       " 0.31641384959220886,\n",
       " 0.30675777792930603,\n",
       " 0.2980964481830597,\n",
       " 0.2832756042480469,\n",
       " 0.2906016409397125,\n",
       " 0.27877742052078247,\n",
       " 0.28391191363334656,\n",
       " 0.2910594642162323,\n",
       " 0.28779152035713196,\n",
       " 0.2862228751182556,\n",
       " 0.29419371485710144,\n",
       " 0.2885968089103699,\n",
       " 0.29176706075668335,\n",
       " 0.2837642729282379,\n",
       " 0.2825590670108795,\n",
       " 0.2836136519908905,\n",
       " 0.29551324248313904,\n",
       " 0.26924580335617065,\n",
       " 0.2857402563095093,\n",
       " 0.2920055091381073,\n",
       " 0.2952550947666168,\n",
       " 0.28085413575172424,\n",
       " 0.2722316086292267,\n",
       " 0.28703582286834717,\n",
       " 0.30024200677871704,\n",
       " 0.27507495880126953,\n",
       " 0.27389177680015564,\n",
       " 0.29165738821029663,\n",
       " 0.2795353829860687,\n",
       " 0.29760968685150146,\n",
       " 0.2676777243614197,\n",
       " 0.2810657024383545,\n",
       " 0.29258814454078674,\n",
       " 0.28253716230392456,\n",
       " 0.2802966833114624,\n",
       " 0.271668940782547,\n",
       " 0.26780635118484497,\n",
       " 0.26888513565063477,\n",
       " 0.26089775562286377,\n",
       " 0.29223495721817017,\n",
       " 0.28402212262153625,\n",
       " 0.2656555473804474,\n",
       " 0.2975133955478668,\n",
       " 0.28430306911468506,\n",
       " 0.28116586804389954,\n",
       " 0.2744358479976654,\n",
       " 0.27448371052742004,\n",
       " 0.2817610502243042,\n",
       " 0.2766990065574646,\n",
       " 0.2977001965045929,\n",
       " 0.28994154930114746,\n",
       " 0.26629823446273804,\n",
       " 0.29749569296836853,\n",
       " 0.2733747661113739,\n",
       " 0.2763279378414154,\n",
       " 0.2715437412261963,\n",
       " 0.28154879808425903,\n",
       " 0.29503846168518066,\n",
       " 0.26763367652893066,\n",
       " 0.2805185914039612,\n",
       " 0.284907728433609,\n",
       " 0.2744303047657013,\n",
       " 0.2935551404953003,\n",
       " 0.27175283432006836,\n",
       " 0.2851785123348236,\n",
       " 0.28744176030158997,\n",
       " 0.27464285492897034,\n",
       " 0.27283602952957153,\n",
       " 0.26252180337905884,\n",
       " 0.2865036725997925,\n",
       " 0.2794140577316284,\n",
       " 0.2578679621219635,\n",
       " 0.27850010991096497,\n",
       " 0.27251216769218445,\n",
       " 0.2562812864780426,\n",
       " 0.27304649353027344,\n",
       " 0.2614207863807678]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loop(model, optimizer, vocab_size, calibration_loader, [calibration_loader, val_loader], max_iters = 200, eval_interval=50, eval_iters=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val': tensor(2.1561)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_loss(model, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
